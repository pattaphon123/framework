# System-wide configuration for Envilink Pipeline Framework
# Environment-specific values are loaded from env_config.yaml (generated by generate_env_config.py)

gcp:
  project_id: '${env.gcp.project_id}'
  region: '${env.gcp.region}'
  zone: '${env.gcp.zone}'

storage:
  raw_bucket: '${env.storage.raw_bucket}'
  staging_bucket: '${env.storage.staging_bucket}'
  config_bucket: '${env.storage.config_bucket}'

# Cloud Composer configuration
composer:
  environment_name: '${env.composer.environment_name}'
  location: '${env.composer.location}'
  dags_bucket: '${env.composer.dags_bucket}'
  
  # DAG generation defaults (applied to all pipelines unless overridden)
  dag_defaults:
    owner: 'airflow'
    retries: 2
    retry_delay_minutes: 5
    retry_exponential_backoff: true
    max_retry_delay_minutes: 30
    execution_timeout_hours: 2
    depends_on_past: false
  
  schedule_defaults:
    start_date: '2024-01-01'
    catchup: false
    max_active_runs: 1

# Dataproc Serverless configuration
dataproc_serverless:
  region: '${env.gcp.region}'
  subnet: '${env.dataproc.subnet}'
  service_account: '${env.dataproc.service_account}'
  runtime_config:
    version: '2.1'
  # Default batch configuration (fallback if not specified in pipeline)
  batch_config:
    spark.executor.instances: '2'
    spark.executor.cores: '4'
    spark.executor.memory: '8g'
    spark.driver.cores: '2'
    spark.driver.memory: '4g'

# Microsoft Teams notifications (applied to all DAGs)
notifications:
  msteams:
    enabled: true
    webhook_url: '${env.msteams.webhook_url}'
    notify_on_success: false
    notify_on_failure: true
    notify_on_retry: false

# Monitoring configuration (applied to all DAGs)
monitoring:
  enable_metrics_collection: true
  metrics_table: '${env.gcp.project_id}.monitoring.pipeline_metrics'

biglake:
  catalog: '${env.biglake.catalog}'
  discovery_dataset: 'discovery_dataset'
  standardized_dataset: 'standardized_dataset'

